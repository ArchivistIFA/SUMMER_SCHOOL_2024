<!DOCTYPE html>
<html>
  <head>
    <title>Digital Summer School 2024: WED04</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="../style.css">  </head>
  <body>
    <textarea id="source">







class: center, middle, titlepage
### WED04: *External Connections.*

---
class: contentpage
### **Agenda**


In this session, we are going to talk about "external connections". Up until now, all our processes have been happening locally, but what if we seek to connect with the outside world?

You will notice that Wikipedia and Wikidata feature very heavily in this section, as they both comprise the largest unified open knowledge resources on the web. 


1. DROID        
    1.1 PRONOM     
2. Wikification of text



---
class: contentpage
### **1. DROID**

DROID is a very interesting tool from the National Archives in the UK which profiles files. It is very useful if you are given a full collection of digital files given to you, and you want to figure out what you have been given.

https://digital-preservation.github.io/droid/

One of these profiles is a connection to PRONOM which is a registry of file formats, also maintained by the UK National Archives.

https://www.nationalarchives.gov.uk/PRONOM/Default.aspx





---
class: contentpage
### **1. DROID**



To run DROID from the command line we are going to want to download the "bin" from the DROID GitHub releases page.

https://github.com/digital-preservation/droid/releases

Once we have unpacked the bin.zip we can run the following to show everything is working.

```sh
java -jar droid-command-line-6.8.0.jar -v
```

And now we can use it on a directory! Here using our scans. This is not especially interesting as the film scans are quite homogenous, but try pointing towards your Downloads directory and seeing what reports you get.

```sh
java -jar droid-command-line-6.8.0.jar -a /home/paulduchesne/Desktop/summer-school-2024/media -o /home/paulduchesne/Desktop/droid_test.csv
```

Just to unpack what is happening here, DROID is processing each file and checking that the "signature" of the file (which we saw earlier) are correct and match the extension - this is to catch any instances where someone has taken an "image.jpg" and renamed as "image.dpx".



What is especially interesting, and the focus of this session, is the PUID column, which is printing the PRONOM identifier.

---
class: contentpage
### **1.1 PRONOM**



If we go across to the PRONOM technical registry, we can look up this identifier and get more contextual information on what this file is.

This exciting to me for two reasons, one it means that archives can start to compare their digital collections with a greater level of detail, ie when I talk about fmt/193 we both know we are talking about the same thing



https://www.wikidata.org/wiki/Q1676669



But also this can link out to playability information. So one of the outstanding issues for an audiovisual archive is the question of how much of my collection is at risk, and these tools are the means of identifying at scale where there are parts of your collection.


Note that I'm really talking about born digital collections here, obviously if you are digitising to a certain spec you should be well aware of the playability of those files.




---
class: contentpage
### **2. Concept Wikification**


The first section covered connecting technical signature externally, but now we can revisit the generated content extraction from the last session and look at a process called Wikification.

This is a very new field for archives, and can be seen as a continuation of the "subject" cataloguing field, but with far greater granularity.

From the previous session we extracted the speech-to-text of our film scan, we can now feed it to an LLM and ask it to match across to Wikipedia entities.

We are in an interesting position where the huge language models seem to be performing very well, the lower level are performing 

two considerations are context aware Wikification, for example if I said "I grew up in Australia and visited Newcastle often", that Newcastle being different to "I grew up in the UK and visited Newcastle often".

Second is that there is a difference between an LLM inventing a good-looking wiki address and knowing whether the wiki entity exists.

<!-- https://en.wikipedia.org/wiki/Newcastle,_New_South_Wales -->










---
class: center, middle, titlepage
### End of Day Three.







    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript"></script>
    <script type="text/javascript">var slideshow = remark.create({ratio: "16:9"});</script>
  </body>
</html>