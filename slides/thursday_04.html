<!DOCTYPE html>
<html>
  <head>
    <title>Digital Summer School 2024: THU04</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="../style.css">  </head>
  <body>
    <textarea id="source">



class: center, middle, titlepage
### THU04: *Multiprocessing, Logging, Containerisation, Orchestration.*

---
class: contentpage
### **Agenda**

This afternoon we are going to look at some more advanced Python topics, such as Multiprocessing and Logging.

Then we are going to look at containerisation and orchestration, which are a jump up in complexity level from what we have covered, but very useful if you want to manage many automated processes.

---
class: contentpage
### **1. Multiprocessing with Python**

Multiprocessing, in this context, refers to the fact that Python runs on a single processor.

What does this mean? A modern computer will generally have multiple "processors" to balance the load of different operations, but systems like Python are only able, by default, to draw upon one processor at a time.

To illustrate this, we can open up our activity monitor and observe activity.

---
class: contentpage
### **1. Multiprocessing with Python**



Depending on your situation, this is a missed opportunity, as you could have an extremely powerful computer where your Python process is only using a small percentage of total possible computational power.

Worth noting that most laptops have limited resources compared to higher end desktop computers, so a possible development pathway is to refine a process on your laptop and then transfer to a serious multicore desktop machine (if one is available to you).


---
class: contentpage
### **1. Multiprocessing with Python**



The goal here is that if a process takes a certain amount of time, if you can efficiently multiprocess on a 64-core machine, the process will run 64 times faster.

There is a big difference between 1 day and 64 days, or 1 year and 64 years!!!



---
class: contentpage
### **1. Multiprocessing with Python**





Python has a handy multiprocessing library called "multiprocessing".

The first thing we would want to do is so how many processors our computer has to give some idea of how we allocate resources.

```python
import multiprocessing
multiprocessing.cpu_count() 
```

Also useful is that the library will automatically make use of as many "CPUs" (or processors) as are available.

---
class: contentpage
### **1. Multiprocessing with Python**


We can now define a task in Python (or a "function"), and then organise for it to run in batches.

The Pool method makes allocating tasks in batches quite easy, but note that all results will be returns when the longest has finished.

```python
import multiprocessing
import time

def my_process(file_name):
    print(f"{file_name} has started.")
    time.sleep(10) 
    print(f"{file_name} has ended.")

if __name__ == "__main__":

    files = ['file_01', 'file_02', 'file_03', 'file_04']
    
    p = multiprocessing.Pool()
    with p:
        p.map(my_process, files)

    print("All processes have finished.")
```


---
class: contentpage
### **1. Multiprocessing with Python**


There are also a few new code components we can see here!




---
class: contentpage
### **1. Multiprocessing with Python**


```python
import multiprocessing
import time

def my_process(file_name):
    print(f"{file_name} has started.")
    time.sleep(10) 
    print(f"{file_name} has ended.")

if __name__ == "__main__":

    files = ['file_01', 'file_02', 'file_03', 'file_04']
    
    p = multiprocessing.Pool()
    with p:
        p.map(my_process, files)

    print("All processes have finished.")
```

The `def name()` construction in Python is what we can define a function, which is a common way to group a block of code we want to reuse to do the same thing over and over, but maybe with different inputs.

This is perfect for multiprocessing, where we want to define a single block of code which gets executed in different processes at the same time.



---
class: contentpage
### **1. Multiprocessing with Python**


```python
import multiprocessing
import time

def my_process(file_name):
    print(f"{file_name} has started.")
    time.sleep(10) 
    print(f"{file_name} has ended.")

if __name__ == "__main__":

    files = ['file_01', 'file_02', 'file_03', 'file_04']
    
    p = multiprocessing.Pool()
    with p:
        p.map(my_process, files)

    print("All processes have finished.")
```

The `if __name__ == "__main__":` part is a common way of making sure that the Python script is being executed directly, and not being imported into other code.

If you recall, a "library" is just someone else's code we have imported, and this is an explicit mechanism to avoid this happening accidentally.



---
class: contentpage
### **1. Multiprocessing with Python**


```python
import multiprocessing
import time

def my_process(file_name):
    print(f"{file_name} has started.")
    time.sleep(10) 
    print(f"{file_name} has ended.")

if __name__ == "__main__":

    files = ['file_01', 'file_02', 'file_03', 'file_04']
    
    p = multiprocessing.Pool()
    with p:
        p.map(my_process, files)

    print("All processes have finished.")
```

If we run this, we will observe that four processes start simultaneously, perform the action which is to "sleep" (do nothing) for ten seconds, and the processes are all returned at the same time.


---
class: contentpage
### **1. Multiprocessing with Python**

Practical: Let us  now write a script to hash all the image files in our film scan, then write a multiprocessing variant and observe the speed difference.

Hint: An easy way to get timestamps is using the datetime library

```python
import datetime

print(datetime.datetime.now())
```


---
class: contentpage
### **1. Multiprocessing with Python**

Solutions!



---
class: contentpage
### **1. Multiprocessing with Python**

Solutions: Hashing all files in film scan sample.



```python
import datetime
import hashlib
import pathlib

start_time = datetime.datetime.now()
files = pathlib.Path.cwd() / 'media'
for file in files.iterdir():
    with open(file, 'rb') as f:  
        file_content = f.read()  
        result = hashlib.md5(file_content).hexdigest()  

end_time = datetime.datetime.now()
print('processing time:', end_time-start_time)
```

---
class: contentpage
### **1. Multiprocessing with Python**


Solutions: Multiprocess hashing all files in film scan sample.


```python
import datetime
import multiprocessing
import time

def my_process(file_name):

    with open(file_name, 'rb') as f:  
        file_content = f.read()  
        result = hashlib.md5(file_content).hexdigest()  

if __name__ == "__main__":
    start_time = datetime.datetime.now()
    files = pathlib.Path.cwd() / 'media'
    files = [x for x in files.iterdir()]

    p = multiprocessing.Pool()
    with p:
        p.map(my_process, files)
    end_time = datetime.datetime.now()

    print('processing time:', end_time-start_time)

```





---
class: contentpage
### **1. Multiprocessing with Python**


Multiprocessing is extremely useful if you have "expensive" operations (like hashing!) which are causing you to wait for your script to finish. 
It does however introduce extra complexity to your scripts, so should only be used if requried.



---
class: contentpage
### **1. Multiprocessing with Python**





~

Logging is an important feature when we build a complex pipeline and something goes wrong, or we even want to check up on where systems are at.

Python has an inbuilt logging feature which is quite similar to the "print" statements, but intended to report activity. Another difference is they are written to disk, whereas print statements are "ephemeral".

Logging is extremely useful if you ever get asked the questions: when did you exactly transcode that file? How long did it take? What was the output?

LOGGING examples.



TODO small logging example

TODO then add logging to previous example


~

Containerisation refers to the process of encapusltaing an enviroment to ensure that systems run predictably. Why would you do this? Quite simply because every computer in the world, once it has been used for a sginicant period of time, becomes its own enviroemnt, Files sit in different locations, applications are installed or removed, etc. There are also inherent differences we have seen on different applications.

Docker is uniqiiouts in this space, but Podman also has some benefits around security considerations.

CONATINER example.


~

The last topic for today is orchestration. We have covered writing and scheduling individual tasks, which is fine if you are running one or two handfuls of processes. Once you scale up to fifty or a hundred distinct processes this can be hard to manage.

What we need then is a system to manage all of our processes, which is generally called an orchestrator. 

This is an ineteresing subject as I believe Joanna and I are both at the point where our own processes have only recently reached this moment, so these tools are new for us as well.

ORCH examples







    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript"></script>
    <script type="text/javascript">var slideshow = remark.create({ratio: "16:9"});</script>
  </body>
</html>